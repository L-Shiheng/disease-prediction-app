import io
import os
import joblib
import torch
import numpy as np
import pandas as pd
import streamlit as st
import pyopenms as oms
import torch.nn as nn
# å¼•å…¥ tempfile å¤„ç†ä¸Šä¼ æ–‡ä»¶
import tempfile 
from jcamp import jcamp_readfile
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, DataLoader

# ==========================================
# 1. å¿…é¡»åŒ…å«çš„æ¨¡å‹ç±»å®šä¹‰
# ==========================================
class BiLSTM(nn.Module):
    def __init__(self, input_size,hidden_size,num_layers,num_classes,BiDirection=True):
        super(BiLSTM, self).__init__()
        self.layer_norm = nn.LayerNorm(input_size) 
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.BiDirection = BiDirection
        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,dropout=0.3,batch_first=True,bidirectional=BiDirection)        
        # å…¨è¿æ¥è¾“å‡ºå±‚
        if self.BiDirection == True:
            self.fc = nn.Linear(hidden_size*2,num_classes)
        else:
            self.fc = nn.Linear(hidden_size,num_classes)

    def forward(self, x):
        # åˆå§‹åŒ–éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€
        if self.BiDirection == True:
            hc0 = self.num_layers*2
        else:
            hc0 = self.num_layers
        h0 = torch.zeros(hc0, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(hc0, x.size(0), self.hidden_size).to(x.device)
        
        # LSTMå‰å‘ä¼ æ’­
        out, (h_n,c_n) = self.lstm(x, (h0, c0))
        
        # å…¨è¿æ¥å±‚é¢„æµ‹
        out = self.fc(out[:, -1, :])
        
        return out

# ==========================================
# 2. æ¨¡å‹é¢„å¤„ç†ç±»å®šä¹‰
# ==========================================
class Data_prepossing:
    def __init__(self,tolerance:float=0.5,SEQ_LENGTH:int=3,SEQ_SIZE:int=80):
        super(Data_prepossing, self).__init__()
        self.tolerance = tolerance
        self.seq_length = SEQ_LENGTH
        self.seq_size = SEQ_SIZE
    
    # æ•°æ®è´¨æ§ï¼Œè´¨è°±å³°å»å™ªï¼Œå»å¹³å¤´å³°
    def noise_removal(self,mass_list,tolerance=0.5):
        total = mass_list.tolist()
        if len(total) < 2: return total # é˜²æ­¢æ•°æ®è¿‡å°‘æŠ¥é”™
        ref_total = total[1:]+[[0,0]]
        new_total = [[r[0]-m[0],r[1]-m[1]] for r,m in zip(ref_total,total)]
        tf = [total[0]]
        for new,ref,to in zip(new_total,ref_total,total):
            if new[0] >= tolerance:
                tf = tf+[ref]
            else:
                if new[1]>=0:
                    tf = tf[:-1]+[ref]+[ref]
                else:
                    tf = tf[:-1]+[to]+[to]
        tf = [m for i,m in enumerate(tf) if m not in tf[:i]]
        return tf

    # ç”Ÿæˆopenmsæ•°æ®æ ¼å¼
    def openms_data_format(self,mass,intensity,decimal=5):
        # è´¨è°±ä¿ç•™
        mz = np.round(mass.values,decimal)
        mz_intensity = intensity.values
        spectrum = oms.MSSpectrum()
        spectrum.set_peaks([mz,mz_intensity])
        spectrum.sortByPosition()
        return spectrum

    # è´¨é‡æ•°å¯¹é½
    def mass_align(self,ref_spectrum,obs_spectrum,tolerance=0.5):
        alignment = []
        spa = oms.SpectrumAlignment()
        p = spa.getParameters()
        # use 0.5 Da tolerance (Note: for high-resolution data we could also 
        # use ppm by setting the is_relative_tolerance value to true)
        p.setValue("tolerance", tolerance)
        p.setValue("is_relative_tolerance", "false")
        spa.setParameters(p)
        # align both spectra
        spa.getSpectrumAlignment(alignment, ref_spectrum, obs_spectrum)
        return alignment

    # æŒ‰å‚æ¯”æ–‡ä»¶_2
    def mass_calculation_ref(self,re_spectrum,ob_spectrum,alignment,decimal=4):
        ref = [i[0] for i in alignment]
        obs = [j[1] for j in alignment]
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
        for i,j in zip(ref,obs):
            if i < len(re_spectrum) and j < len(ob_spectrum):
                ob_spectrum.iloc[j, 0] = re_spectrum.iloc[i, 0]
        return re_spectrum,ob_spectrum
    
    def load_imputer(self):
        # ä¿®æ”¹ä¸ºç›¸å¯¹è·¯å¾„
        scaler_path = 'imputer_scaler_model.pkl' 
        if not os.path.exists(scaler_path):
            st.error(f"âŒ æ‰¾ä¸åˆ°é¢„å¤„ç†æ–‡ä»¶ï¼š{scaler_path}")
            return None
        try:
            # ä½¿ç”¨ joblib åŠ è½½ sklearn çš„å¯¹è±¡
            scaler = joblib.load(scaler_path)
            return scaler
        except Exception as e:
            st.error(f"é¢„å¤„ç†å™¨åŠ è½½å‡ºé”™: {e}")
            return None
    
    # æ•°æ®å¯¹é½å’Œæ•´åˆ
    # uploaded_file å®é™…ä¸Šæ˜¯ UploadedFile å¯¹è±¡åˆ—è¡¨ï¼Œä¸æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
    def prediction_pretreatment(self,uploaded_files,ages:list[int],genders:list[int]):
        sample_name = []
        
        # è¯»å–åŒçº§ç›®å½•ä¸‹çš„ excel
        if not os.path.exists('train_target.xlsx'):
            st.error("âŒ æ‰¾ä¸åˆ° train_target.xlsxï¼Œè¯·ç¡®ä¿å·²ä¸Šä¼ åˆ° GitHub/äº‘ç«¯")
            return None, None
            
        thyroid_train = pd.read_excel('train_target.xlsx')
        prim = thyroid_train.iloc[:,0:2]
        
        DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
        
        # æ•°æ®å¯¹é½
        for i, file in enumerate(uploaded_files):
            # ==============================================
            # ğŸ› ï¸ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ tempfile å¤„ç†å†…å­˜æ–‡ä»¶
            # ==============================================
            file_name = file.name
            sample_name.append(file_name)
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            suffix = ".jdx" if not file_name.endswith(".jdx") else ""
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix + ".jdx") as tmp_file:
                tmp_file.write(file.getvalue()) # å†™å…¥æ•°æ®
                tmp_file_path = tmp_file.name

            try:
                # è¯»å–ä¸´æ—¶æ–‡ä»¶
                jdxfile = jcamp_readfile(tmp_file_path)
                indata = np.vstack((jdxfile['x'],jdxfile['y'])).T
                
                # å»é™¤å™ªå£°
                denoise = self.noise_removal(indata,tolerance=self.tolerance) 
                framefile = pd.DataFrame(denoise,columns=['mass',file_name])
                
                # æ•°æ®éªŒè¯
                ref_spectrum = self.openms_data_format(prim.mass,prim.iloc[:,1])
                obs_spectrum = self.openms_data_format(framefile.mass,framefile.iloc[:,1])
                alignment = self.mass_align(ref_spectrum,obs_spectrum,tolerance=self.tolerance)
                
                # æ•°æ®æ•´åˆ
                r_spectrum,o_spectrum = self.mass_calculation_ref(prim,framefile,alignment)
                # left merge ä¿è¯ä¿ç•™ reference çš„ mass
                prim = pd.merge(prim, o_spectrum, how='left', on='mass') 
                
            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {e}")
                st.stop()
            finally:
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(tmp_file_path):
                    os.remove(tmp_file_path)
        
        # åˆ å»è¾…åŠ©æ•°æ® (å»æ‰å‰ä¸¤åˆ—ï¼šmass å’Œ train_target çš„ intensity)
        # æ³¨æ„ï¼šprim çš„ç»“æ„å˜æˆäº† [mass, ref_intensity, file1, file2...]
        # æ‰€ä»¥è¦ drop å‰ä¸¤åˆ—
        try:
            prediction = prim.iloc[:, 2:].T # è½¬ç½®åï¼šè¡Œæ˜¯æ ·æœ¬ï¼Œåˆ—æ˜¯ç‰¹å¾
            
            # æ£€æŸ¥ç‰¹å¾æ•°é‡æ˜¯å¦åŒ¹é… scaler
            scaler = self.load_imputer()
            if scaler:
                # å¦‚æœæœ‰ç¼ºå¤±å€¼ï¼ˆå› ä¸º merge left å¯èƒ½äº§ç”Ÿ NaNï¼‰ï¼Œå…ˆå¡«å…… 0
                prediction = prediction.fillna(0)
                prediction = scaler.transform(prediction)
            
            # å¹´é¾„å’Œæ€§åˆ«æ•°æ®æ•´åˆ
            buckets=[0,20,40,60,80]  
            age_bucket = np.digitize(ages, buckets, right=False) - 1
            
            # ğŸ› ï¸ å…³é”®ä¿®å¤ï¼šæ€§åˆ«é€»è¾‘
            # UI è¾“å…¥ï¼š1 (ç”·), 0 (å¥³)
            # åŸé€»è¾‘ï¼š'male' -> [0, 1]
            # æ–°é€»è¾‘ï¼š1 -> [0, 1], 0 -> [1, 0]
            categoricals = np.array([[0,1] if m == 1 else [1,0] for m in genders]).T
            
            totals = np.vstack((prediction.T, age_bucket.astype('float32'), categoricals))
            
            # è½¬æ¢ä¸ºtensor
            prediction_tensor = torch.tensor(totals.T, dtype=torch.float32)
            
            # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é… reshape
            expected_features = self.seq_length * self.seq_size
            if prediction_tensor.shape[1] != expected_features:
                 st.error(f"ç»´åº¦é”™è¯¯ï¼šå¤„ç†åç‰¹å¾æ•°ä¸º {prediction_tensor.shape[1]}ï¼Œä½†æ¨¡å‹éœ€è¦ {expected_features}")
                 st.info("æç¤ºï¼šè¯·æ£€æŸ¥ train_target.xlsx çš„è¡Œæ•°æˆ–é¢„å¤„ç†é€»è¾‘æ˜¯å¦ä¸è®­ç»ƒæ—¶ä¸€è‡´ã€‚")
                 st.stop()

            prediction_seq = prediction_tensor.view(-1, self.seq_length, self.seq_size).to(DEVICE)
            
            return prediction_seq, sample_name
            
        except Exception as e:
            st.error(f"æ•°æ®æ•´åˆé˜¶æ®µå‡ºé”™: {e}")
            return None, None

# ==========================================
# 3. Streamlit ç•Œé¢åŠæ¨¡å‹ä¸Šè½½
# ==========================================
@st.cache_resource
def load_deep_model():
    # ç›¸å¯¹è·¯å¾„
    model_path = 'PDD.pth'  
    if not os.path.exists(model_path):
        return None
    try:
        model = torch.load(model_path, map_location='cpu', weights_only=False)
        model.eval()
        return model
    except Exception as e:
        st.error(f"LSTM æ¨¡å‹åŠ è½½å‡ºé”™: {e}")
        return None

# åˆå§‹åŒ–
st.title("åŸºäºDPiMSå’Œæ·±åº¦å­¦ä¹ çš„ç‰™å‘¨ç—…è¯Šæ–­ç³»ç»Ÿ")

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
model = load_deep_model()
if model is None:
    st.error("âŒ æ‰¾ä¸åˆ° PDD.pthï¼Œè¯·ç¡®ä¿æ–‡ä»¶å·²ä¸Šä¼ ï¼")
    st.stop()

uploaded_files = st.file_uploader("è¯·ä¸Šä¼  JDX è´¨è°±æ–‡ä»¶", type=["jdx", "dx"], accept_multiple_files=True)

ages = []
genders = []

if uploaded_files:
    st.divider()
    st.write("### 1. å¡«å†™æ‚£è€…ä¿¡æ¯")
    # ä½¿ç”¨ columns å¸ƒå±€æ›´ç´§å‡‘
    for i, file in enumerate(uploaded_files):
        c1, c2, c3 = st.columns([2, 1, 1])
        with c1:
            st.text(f"ğŸ“ {file.name}")
        with c2:
            age = st.number_input(f"å¹´é¾„", min_value=0, max_value=120, value=25, key=f"age_{i}", label_visibility="collapsed")
            ages.append(age)
        with c3:
            # è¿™é‡Œ 1=ç”·, 0=å¥³
            gender = st.selectbox(f"æ€§åˆ«", options=[1, 0], format_func=lambda x: "ç”·" if x == 1 else "å¥³", key=f"gender_{i}", label_visibility="collapsed")
            genders.append(gender)
    
    st.divider()
    if st.button("å¼€å§‹è¯Šæ–­"):
        with st.spinner("æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†å’Œåˆ†æ..."):
            # æ•°æ®é¢„å¤„ç†
            # ç¡®ä¿ SEQ_SIZE å’Œ SEQ_LENGTH ä¸ä½ è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼
            prepossessor = Data_prepossing(SEQ_LENGTH=3, SEQ_SIZE=80)
            
            input_tensor, col_name = prepossessor.prediction_pretreatment(uploaded_files, ages, genders)
            
            if input_tensor is not None:
                DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
                model = model.to(DEVICE)

                # é¢„æµ‹
                with torch.no_grad():
                    output = model(input_tensor)
                    probabilities = torch.softmax(output, dim=1)
                    predicted_class = torch.argmax(probabilities, dim=1)
                    confidence = torch.max(probabilities, dim=1)[0].cpu()

                # ç»“æœå±•ç¤º
                st.subheader("ğŸ”® è¯Šæ–­ç»“æœ")
                group_name = ['å¥åº·', 'ç‰™å‘¨ç‚+ç³–å°¿ç—…', 'ç‰™å‘¨ç‚']
                
                # åˆ›å»ºç»“æœè¡¨æ ¼
                results = []
                for col, m, n in zip(col_name, predicted_class, confidence):
                    res_dict = {
                        "æ ·æœ¬åç§°": col,
                        "è¯Šæ–­ç»“æœ": group_name[m],
                        "ç½®ä¿¡åº¦": f"{n.item():.2%}"
                    }
                    results.append(res_dict)
                
                st.table(results)
